{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "To Do:\n",
    "- get fMRI data going.. \n",
    "- functionize... \n",
    "- set-up simulations.. what I want to look at etc..(save the results out into dataframe)..each time adding a row.. \n",
    "- then I can just add some more to the simulations.. ..e.g. \n",
    "- \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#### loop\n",
    "#### sim data.. \n",
    "#### fit.. \n",
    "#### calc performance... (time-series prediction.. Beta estimation.. from true beta's using training set..## \n",
    "#### save into dataframe... \n",
    "####\n",
    "\n",
    "#### look at my beta's for the whole simulation... XXX... \n",
    "#### and my signal to noise.. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "breakpoint": false
    }
   },
   "source": [
    "## Notation Used Throughout Paper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Setup\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "we have q yi = aixi + Ei\n",
    "so stacking, \n",
    "we have \n",
    "\n",
    "$$ Y = AX + E  (qxp)(1xp)(px1) $$\n",
    "for a single time-point\n",
    "\n",
    "\n",
    "See section 2.1\n",
    "\n",
    "**Errors are iid across observations, though can be correlated across y's, e.g.(spatially) (is this assumption ok for fMRI)?**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "breakpoint": false
    }
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#####Covariance\n",
    "\n",
    "1. **noise covariance** between y's.  (dominates in this situation, as seen by upward cloud)\n",
    "if the y1 and y3 are both up and down together, why does this help?\n",
    "\n",
    "2. **signal covariance** (e.g. shared effects). \n",
    "if they share similar coefficients, but have IID noise. Then its like averaging to estimate the same sample mean. \n",
    "They are correlated because they share the same dependence on the predictor variable xb. \n",
    "\n",
    "Wait there is also $X X^T=V$; is this the signal covariance? Or perhaps this along with the magnitude of B. \n",
    "\n",
    "\n",
    "## OLS\n",
    "\n",
    "\n",
    "## Shrinking\n",
    "\n",
    "Now, however we know that if we didnt stack, we can get a better prediction on y by shrinking the y^ by K. \n",
    "So at the minimunm\n",
    "we could have \n",
    "Yhat = AhatX\n",
    "\n",
    "$$ \\tilde Y = S \\hat B X $$\n",
    "\n",
    "Ytild = B Yhat, where B is diagnol with K. \n",
    "\n",
    "### Optimal \n",
    "\n",
    "\n",
    "\n",
    "Deriving form the linear model\n",
    "\n",
    "Skip straight to expectation over possible construction sets. \n",
    "\n",
    "**want** multi-response coefficient vector for population regression\n",
    "\n",
    "**wait are these vectors?? or matrices**\n",
    "\n",
    "**Expectation is over the population distribution of x and e. Y is the true variable Y**\n",
    "\n",
    "\n",
    "which is the \n",
    "$$ E(Y \\hat Y^T) E(\\hat Y \\hat Y^T)^{-1}$$\n",
    "\n",
    "The covariance / variance. Like the coefficient \n",
    "\n",
    "**why is this qxq?**\n",
    "\n",
    "$$ Ytest = M \\hat Y $$ \n",
    "\n",
    "**assume the sample means and covariance are equivalent to the population.. like conditioning on design matrix**\n",
    "\n",
    "\n",
    "##### Theoretically Best S (switch to B).. need to do for single y.. \n",
    "\n",
    "S = F(F + 1/n Sigma)-1\n",
    "\n",
    "Where F is the 'signal matrix'\n",
    "BXX^TB\n",
    "\n",
    "XX^T\n",
    "is \n",
    "\n",
    "This matrix is not the same as the usual covariance of X.. which is 2x2\n",
    "\n",
    "This is outerproduct which is what? \n",
    "Well in each entry is each observation \n",
    "well the first entry is x1t=0 * x1t=0 + x2t=0 * x2t=0\n",
    "So each entry is the sum of some time point in x1 multipled by some other time point in x1 + the same for x2. \n",
    "Not intuitive at all. \n",
    "\n",
    "OH wait never mind.. its going to be pxp.. not sure why he put xx^T. \n",
    "\n",
    "Ahh.. we are working with every single time point in His Notes. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Canonical Correlations\n",
    "\n",
    "The key to our data is that we have q+p random variables. Let's say 3 responses and 2 predictors. You have two random vectors y_i^T (y1,y2,..) and x_i^T (x1,x2,..). \n",
    "\n",
    "Every observation (y1,y2,y3,x1,x2) is a realization from this joint distribution. \n",
    "\n",
    "We've specified how the y's relate to the x's. And we take the x's as fixed..? Or can we? \n",
    "Let's imagine their not. \n",
    "\n",
    "Then we want to know the covariance of \n",
    "Cov(bold y, bold x). \n",
    "E(E(y)-y)(E(x)-x)^T). with vectors. \n",
    "Now we substitute expectation of E(y) with a mean vector. and we can subtract that out. \n",
    "Then we have YX^T. To estimate our full coviarance matrix. pxq. \n",
    "\n",
    "\n",
    "Compare to situation where we just have two random variables. \n",
    "cov(y,x). \n",
    "This is just E(E(y)-y)(E(x)-x)). With scalar quantities. \n",
    "To estimate this we might have multiple pairs (x,y) (x,y), so we take E(y) to be bar y, and subtract that out. Then we take the observations y [y .. yi] as a vector and do inner product with x observations. \n",
    "This estimates our single number cov(y,x). \n",
    "\n",
    "\n",
    "\n",
    "The cross-covariance is a matrix of nxm, which is the covariance of each y with each x. Because they are random variables you can imagine every entry in each vector as a vector itself (all the possible realizations, and its expectation.. so its the expectation around the expectation squared). \n",
    "\n",
    "This again is for a single time point?... \n",
    "Or do I use the fact that I have multiple observations for each yq... i..100 time points. \n",
    "Assuming they all represent the same yi.. then I can. \n",
    "Right? \n",
    "Or do I do this for each time point... and get a yx^T cross covariance matrix.. no that'd be silly. \n",
    "\n",
    "#### Solution\n",
    "\n",
    "\n",
    "we want $ E(yy^T)^{-1}E(yx^T)E(xx^T)^{-1}E(xy^T) $\n",
    "\n",
    "We estimate each of the covariance's, by matrix multiplication on centered matrices. \n",
    "\n",
    "Then we do eigenvalue decomposition on this matrix. \n",
    "\n",
    "$ Q = E(yy^T)^{-1}E(yx^T)E(xx^T)^{-1}E(xy^T) = T^T C^2 T^{-T} $\n",
    "\n",
    "\n",
    "We need the sample based estiamte of Q\n",
    "\n",
    "$ \\hat Q = E(Y^TY)^{-1}E(Y^TX)E(X^TX)^{-1}E(X^TY) = T^T C^2 T^{-T} $\n",
    "\n",
    "In each of these it should be 3x3 3x2 ... .Basically we want the covariance of each x with each y. and to estimate that we sum across 40 observations. \n",
    "\n",
    "The T and C are for the y canonical correlations. \n",
    "\n",
    "**Can we just use $ \\hat Q$ **\n",
    "\n",
    "\n",
    "\n",
    "### Cross Validation\n",
    "\n",
    "\n",
    "\n",
    "Canonical Correations\n",
    "\n",
    "#### Very Similar to Reduced Rank Regression\n",
    "can be seen as a continuous version of it. \n",
    "\n",
    "\n",
    "### Interpretation... of Beta's?\n",
    "\n",
    "\"The solution involves canonical correlation analysis, and combines\n",
    "information from all of the q response variables into r canonical response variates that have\n",
    "the highest canonical correlation with the corresponding predictor canonical variates. As in\n",
    "the case of principal components regression, the interpretation of the reduced rank model is\n",
    "typically impossible in terms of the original predictors and responses.\"\n",
    "\n",
    "\n",
    "### Mscl\n",
    "To directly exploit the correlation in the response variables to improve prediction performance,\n",
    "a method called Curds and Whey (C&W) was proposed by Breiman and Friedman\n",
    "(1997). C&W predicts the multivariate response with an optimal linear combination of the\n",
    "ordinary least squares predictors. The C&W linear predictor has the form Y˜ = Yˆ\n",
    "OLSM,\n",
    "where M is a q × q shrinkage matrix estimated from the data. This method exploits correlation\n",
    "in the responses arising from shared random predictors as well as correlated errors.\n",
    "\n",
    "\n",
    "\"\"\"MRCR\"\"\"\n",
    "To directly exploit the correlation in the response variables to improve prediction performance,\n",
    "a method called Curds and Whey (C&W) was proposed by Breiman and Friedman\n",
    "(1997). C&W predicts the multivariate response with an optimal linear combination of the\n",
    "ordinary least squares predictors. The C&W linear predictor has the form Y˜ = Yˆ OLSM,\n",
    "where M is a q × q shrinkage matrix estimated from the data. This method exploits correlation\n",
    "in the responses arising from shared random predictors as well as correlated errors.\n",
    "In this article, we propose a method that combines some of the strengths of the estimators\n",
    "discussed above to improve prediction in the multivariate regression problem while\n",
    "allowing for interpretable models in terms of the predictors. We reduce the number of\n",
    "parameters using the lasso penalty on the entries of B while accounting for correlated errors.\n",
    "We accomplish this by simultaneously optimizing (1.1) with penalties on the entries\n",
    "of B and \u0004. We call our new method multivariate regression with covariance estimation\n",
    "(MRCE)\n",
    "\n",
    "We also note that the use of the lasso penalty on the entries of \u0005 has been considered by\n",
    "several authors in the context of covariance estimation (Yuan and Lin 2007; d’Aspremont,\n",
    "Banerjee, and El Ghaoui 2008; Friedman, Hastie, and Tibshirani 2008; Rothman et al.\n",
    "2008). However, here we use it in the context of a regression problem, thus making it an\n",
    "example of what one could call supervised covariance estimation: the covariance matrix\n",
    "here is estimated in order to improve prediction, rather than as a stand-alone parameter.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Their prediction error is about about XB..\n",
    "Following Breiman and Friedman (1997), the mean-squared\n",
    "prediction error of response i, for a particular method m, is\n",
    "\n",
    "##### Need to add Covariance of X into the estimationr errror.. !!!\n",
    "where V is the covariance matrix of X. We compare the average prediction error\n",
    "of each method normalized by the OLS average prediction error.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## What CCA does..\n",
    "\n",
    "Matrices are a convenient way to write down a transformation, especially linear (though some nonlinear affine can be too). All have to be within a space (defined by bases? like 001,010,100). Then the result of a matrix multiplication times a vector is to move that vector in the space. A shearing is easiest to understand. If you shear just in the x direction the matrix is [1,k; 0,1]x[x,y]. The y component stays the same, but the x component is has an additional ky. \n",
    "\n",
    "Eigenvectors are directions or points in the space (ie. vectors) that would not change coordinates under the transformation. So they are eigenvectors relative to a specific transformation. Rotation in 2D does not have any. Rotation in 3D has just 1 eigenvector in the axis of rotation. Think north pole doesnt change location in xyz space as earth spins. \n",
    "\n",
    "So now to CCA; the eigendecomposition of the matrix Q (RRRR) is useful because it illustrates what's really happening behind the scenes when we multiply by Q. So first off, what is Q. Its the cross? covariance of X and Y. So if we put them in the same space ... wait.. \n",
    "Covariance as Transform (white noise into what we see). Then eigenvector of the covariance matrix are direction of maximal spread….\n",
    "So now we think about that in terms of two variables.. \n",
    "We want to find the direction of maximal spread between y and x. In fmri, if all responses share a common signal.. this will likely be a large direction in the space? is this true? \n",
    "So we find the directions in combined y and x space (I think).. of maximal variation. \n",
    "Then the eigenvalue shrinks the value by how large the variance is in that direction. \n",
    "So what if we just multipleid our data by our XX matrix.. \n",
    "\n",
    "Well isnt this what linear regression does?..Yes except it only has Y variable. Whereas we take advantage of all at once.. \n",
    "\n",
    "eigendecomp.. just makes it easier to understand. first rewrite in eigenspace.. what the coordinates are. I think. Then shrink by the eigenvalue.. Then transform back. Easier than just a bunch of \n"
   ]
  }
 ],
 "metadata": {
  "css": [
   ""
  ],
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
